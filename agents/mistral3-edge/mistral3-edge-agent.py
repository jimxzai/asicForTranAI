#!/usr/bin/env python3
"""
Mistral 3 Edge Agent - Local Inference for Three-Books Annotations
ç”¨æ³•ï¼špython mistral3-edge-agent.py "ä»Šå¤©è¯»ã€Šå­™å­Â·å§‹è®¡ç¯‡ã€‹ï¼Œæˆ‘çš„å¿ƒå¾—æ˜¯â€¦â€¦"

Author: Jim Xiao
Date: 2025-12-03
Model: ministral-3:8b (primary) / deepseek-r1:8b (fallback)
Platform: Ollama (Edge Deployment)

ç‰¹ç‚¹ Features:
- ğŸš€ Edge/Local æ¨ç†ï¼ˆæ— éœ€äº‘ç«¯ï¼Œéšç§ä¼˜å…ˆï¼‰
- ğŸ§  Ministral 3:8bï¼ˆ256K ä¸Šä¸‹æ–‡ï¼Œå¤šè¯­è¨€ï¼Œè§†è§‰èƒ½åŠ›ï¼‰
- ğŸ”„ è‡ªåŠ¨é™çº§åˆ° DeepSeek-R1ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰
- ğŸ“Š æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼ˆvs Llama-3.3-70B @ Groqï¼‰
"""

import os
import sys
import json
import subprocess
from datetime import datetime
from pathlib import Path
from typing import Optional, Tuple

# Repository paths
REPO_ROOT = Path(__file__).parent.parent.parent
ANNOTATIONS_DIR = REPO_ROOT / "three-books-ai-annotations"
BENCHMARK_DIR = REPO_ROOT / "agents" / "mistral3-edge" / "benchmarks"

# Model priority list (æŒ‰ä¼˜å…ˆçº§æ’åº)
MODEL_PRIORITY = [
    "ministral-3:8b",      # ä¼˜å…ˆï¼šMistral 3 Edge (éœ€ Ollama 0.13.1+)
    "ministral-3:14b",     # å¤‡é€‰ï¼šæ›´å¼ºæ¨ç†ï¼ˆå¦‚æœèµ„æºè¶³å¤Ÿï¼‰
    "deepseek-r1:8b",      # é™çº§ï¼šå½“å‰å¯ç”¨çš„æ¨ç†ä¼˜åŒ–æ¨¡å‹
    "gpt-oss:20b",         # é™çº§ï¼šé€šç”¨æ¨¡å‹
]


def check_ollama_installed() -> bool:
    """æ£€æŸ¥ Ollama æ˜¯å¦å®‰è£…"""
    try:
        result = subprocess.run(
            ["ollama", "--version"],
            capture_output=True,
            text=True,
            timeout=5
        )
        return result.returncode == 0
    except (FileNotFoundError, subprocess.TimeoutExpired):
        return False


def get_available_models() -> list[str]:
    """è·å–å½“å‰å¯ç”¨çš„ Ollama æ¨¡å‹"""
    try:
        result = subprocess.run(
            ["ollama", "list"],
            capture_output=True,
            text=True,
            timeout=10
        )

        if result.returncode != 0:
            return []

        # è§£æè¾“å‡ºï¼Œæå–æ¨¡å‹åç§°
        lines = result.stdout.strip().split('\n')
        if len(lines) <= 1:  # åªæœ‰æ ‡é¢˜è¡Œ
            return []

        models = []
        for line in lines[1:]:  # è·³è¿‡æ ‡é¢˜
            parts = line.split()
            if parts:
                models.append(parts[0])  # ç¬¬ä¸€åˆ—æ˜¯æ¨¡å‹å

        return models

    except Exception as e:
        print(f"âš ï¸  è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
        return []


def select_best_model() -> Optional[str]:
    """æ ¹æ®ä¼˜å…ˆçº§å’Œå¯ç”¨æ€§é€‰æ‹©æœ€ä½³æ¨¡å‹"""
    available = get_available_models()

    print(f"ğŸ“‹ å½“å‰å¯ç”¨æ¨¡å‹: {', '.join(available) if available else '(æ— )'}")
    print()

    for model in MODEL_PRIORITY:
        if model in available:
            print(f"âœ… é€‰æ‹©æ¨¡å‹: {model}")
            return model

    # å¦‚æœä¼˜å…ˆåˆ—è¡¨ä¸­éƒ½æ²¡æœ‰ï¼Œå°è¯•æ‹‰å– Ministral 3
    print("âš ï¸  ä¼˜å…ˆæ¨¡å‹ä¸å¯ç”¨ï¼Œå°è¯•æ‹‰å– ministral-3:8b...")
    print("ğŸ’¡ è¿™éœ€è¦ Ollama v0.13.1+ï¼Œå½“å‰å¯èƒ½å¤±è´¥")
    print()

    try:
        result = subprocess.run(
            ["ollama", "pull", "ministral-3:8b"],
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )

        if result.returncode == 0:
            print("âœ… ministral-3:8b æ‹‰å–æˆåŠŸï¼")
            return "ministral-3:8b"
        else:
            print(f"âŒ æ‹‰å–å¤±è´¥: {result.stderr}")
            print()
            print("=" * 60)
            print("ğŸ”§ è§£å†³æ–¹æ¡ˆï¼šå‡çº§ Ollama åˆ° v0.13.1+")
            print("=" * 60)
            print("1. è®¿é—®: https://github.com/ollama/ollama/releases")
            print("2. ä¸‹è½½æœ€æ–° pre-release (v0.13.1+)")
            print("3. å®‰è£…åè¿è¡Œ: ollama pull ministral-3:8b")
            print()
            print("ğŸ”„ å½“å‰å°†ä½¿ç”¨å¤‡é€‰æ¨¡å‹...")
            print()

            # è¿”å›ç¬¬ä¸€ä¸ªå¯ç”¨çš„å¤‡é€‰æ¨¡å‹
            if available:
                fallback = available[0]
                print(f"âœ… ä½¿ç”¨å¤‡é€‰: {fallback}")
                return fallback
            else:
                return None

    except subprocess.TimeoutExpired:
        print("âŒ æ‹‰å–è¶…æ—¶")
        return None
    except Exception as e:
        print(f"âŒ æ‹‰å–å¤±è´¥: {e}")
        return None


def call_ollama(model: str, prompt: str, temperature: float = 0.7) -> Tuple[str, dict]:
    """è°ƒç”¨ Ollama æœ¬åœ°æ¨ç†"""

    print(f"ğŸ¤– è°ƒç”¨æ¨¡å‹: {model}")
    print(f"ğŸŒ¡ï¸  æ¸©åº¦: {temperature}")
    print(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
    print()

    start_time = datetime.now()

    try:
        # ä½¿ç”¨ ollama run å‘½ä»¤ï¼ˆæ”¯æŒæµå¼è¾“å‡ºï¼‰
        process = subprocess.Popen(
            ["ollama", "run", model],
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True
        )

        # DeepSeek-R1 æ˜¯æ¨ç†æ¨¡å‹ï¼Œéœ€è¦æ›´é•¿æ—¶é—´ï¼ˆ10åˆ†é’Ÿï¼‰
        # Mistral 3 åˆ™æ›´å¿«ï¼ˆ3-5åˆ†é’Ÿï¼‰
        timeout = 600 if "deepseek" in model.lower() else 300
        stdout, stderr = process.communicate(input=prompt, timeout=timeout)

        if process.returncode != 0:
            print(f"âŒ æ¨ç†å¤±è´¥: {stderr}")
            return None, {}

        elapsed = (datetime.now() - start_time).total_seconds()

        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        output_tokens = len(stdout.split())  # ç²—ç•¥ä¼°è®¡
        tokens_per_sec = output_tokens / elapsed if elapsed > 0 else 0

        metadata = {
            "model": model,
            "elapsed_seconds": round(elapsed, 2),
            "output_tokens_estimated": output_tokens,
            "tokens_per_second": round(tokens_per_sec, 2),
            "timestamp": datetime.now().isoformat()
        }

        print(f"âœ… æ¨ç†å®Œæˆ")
        print(f"â±ï¸  è€—æ—¶: {elapsed:.2f}ç§’")
        print(f"âš¡ é€Ÿåº¦: ~{tokens_per_sec:.1f} tokens/ç§’")
        print()

        return stdout.strip(), metadata

    except subprocess.TimeoutExpired:
        process.kill()
        timeout_str = "10åˆ†é’Ÿ" if "deepseek" in model.lower() else "5åˆ†é’Ÿ"
        print(f"âŒ æ¨ç†è¶…æ—¶ï¼ˆ{timeout_str}ï¼‰")
        if "deepseek" in model.lower():
            print("ğŸ’¡ DeepSeek-R1 æ˜¯æ¨ç†æ¨¡å‹ï¼ˆchain-of-thoughtï¼‰ï¼Œå¤„ç†é•¿æ–‡æœ¬è¾ƒæ…¢")
            print("   å»ºè®®ï¼šç­‰å¾… Mistral 3 å¯ç”¨ï¼Œæˆ–ç¼©çŸ­è¾“å‡ºè¦æ±‚")
        return None, {}
    except Exception as e:
        print(f"âŒ æ¨ç†å¤±è´¥: {e}")
        return None, {}


def generate_sun_tzu_annotation(model: str, user_note: str) -> Tuple[Optional[str], dict]:
    """ç”Ÿæˆã€Šå­™å­å…µæ³•ã€‹AIæ—¶ä»£æ³¨ç–ï¼ˆè¾¹ç¼˜æ¨ç†ç‰ˆï¼‰"""

    prompt = f"""ä½ ç°åœ¨æ˜¯ã€ŒAIæ—¶ä»£å­™å­å…µæ³•æ³¨ç–å¤§å¸ˆã€ï¼Œä»»åŠ¡æ˜¯ï¼š

## ç”¨æˆ·å¿ƒå¾—ï¼š
{user_note}

## ä½ çš„ä»»åŠ¡ï¼š

1. **åˆ¤æ–­ç« èŠ‚**: è‡ªåŠ¨è¯†åˆ«å±äºã€Šå­™å­å…µæ³•ã€‹å“ªä¸€ç¯‡ï¼ˆå¦‚ï¼šå§‹è®¡ç¯‡ã€ä½œæˆ˜ç¯‡ã€è°‹æ”»ç¯‡ç­‰ï¼‰
2. **å¼•ç”¨åŸæ–‡**: æä¾›ç›¸å…³åŸæ–‡ï¼ˆä¸­è‹±å¯¹ç…§ï¼‰
3. **ç»å…¸æ³¨ç–**: å¼•ç”¨æ›¹æ“æ³¨ã€æœç‰§æ³¨ã€æç­Œæ³¨ç­‰
4. **æ·±åº¦è§£è¯»**: 2000-3000å­—åˆ†æï¼ˆä¸­è‹±åŒè¯­ï¼‰
5. **2025 AIæˆ˜ä¾‹å¯¹ç…§**:
   - Groq vs Nvidiaï¼ˆLPU vs GPUï¼Œ"å…µè´µç¥é€Ÿ"ï¼‰
   - Mistral 3 vs Llama 3ï¼ˆè¾¹ç¼˜éƒ¨ç½²ï¼Œ"ä¸æˆ˜è€Œå±ˆäººä¹‹å…µ"ï¼‰
   - AGIå®‰å…¨åšå¼ˆï¼ˆOpenAI vs Anthropicï¼Œ"çŸ¥å½¼çŸ¥å·±"ï¼‰

## è¾“å‡ºæ ¼å¼ï¼ˆMarkdownï¼‰ï¼š

# ã€Šå­™å­å…µæ³•Â·[ç¯‡å]ã€‹- 2025 AIæ—¶ä»£æ³¨ç–

**æ—¥æœŸ**: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}
**æ¨¡å‹**: Mistral 3 Edge (è¾¹ç¼˜æ¨ç†)

## ä¸€ã€åŸæ–‡å¼•ç”¨

ã€ä¸­æ–‡ã€‘
[åŸæ–‡]

ã€Englishã€‘
[Translation]

## äºŒã€å†ä»£æ³¨ç–

- **æ›¹æ“æ³¨**: [å¼•ç”¨]
- **æœç‰§æ³¨**: [å¼•ç”¨]
- **ç°ä»£è§£**: [å¼•ç”¨]

## ä¸‰ã€æ·±åº¦è§£è¯»ï¼ˆ2000å­—ï¼Œä¸­è‹±åŒè¯­ï¼‰

### æˆ˜ç•¥å±‚é¢åˆ†æ

[ä½ çš„æ·±åº¦åˆ†æ...]

### æˆ˜æœ¯å±‚é¢æ˜ å°„

[å…·ä½“æˆ˜æœ¯...]

## å››ã€2025 AIæ—¶ä»£å¯¹ç…§

### æ¡ˆä¾‹1: Groq vs Nvidia - "å…µè´µç¥é€Ÿ"
- **å¤ä¹‰**: [è§£é‡Š]
- **ä»Šæ˜ **: Groq LPU æ¨ç†é€Ÿåº¦ï¼ˆ500 tokens/sï¼‰vs Nvidia GPUï¼ˆ50-100 tokens/sï¼‰
- **å¯ç¤º**: [...]

### æ¡ˆä¾‹2: Mistral 3 è¾¹ç¼˜éƒ¨ç½² - "ä¸æˆ˜è€Œå±ˆäººä¹‹å…µ"
- **å¤ä¹‰**: [è§£é‡Š]
- **ä»Šæ˜ **: Ministral 3ï¼ˆ3-14Bï¼‰åœ¨ Jetson è¾¹ç¼˜è®¾å¤‡æ¨ç†ï¼Œæ— éœ€äº‘ç«¯"äº¤æˆ˜"
- **å¯ç¤º**: [...]

### æ¡ˆä¾‹3: AGIå®‰å…¨ - "çŸ¥å½¼çŸ¥å·±ï¼Œç™¾æˆ˜ä¸æ®†"
- **å¤ä¹‰**: [è§£é‡Š]
- **ä»Šæ˜ **: [...]
- **å¯ç¤º**: [...]

## äº”ã€å¯ç¤ºä¸æ€è€ƒï¼ˆ300å­—ï¼‰

[æ€»ç»“...]

---

**æ ‡ç­¾**: #å­™å­å…µæ³• #AIæ—¶ä»£ #è¾¹ç¼˜AI #Mistral3 #å¼€æºæ¨¡å‹

è¯·ç›´æ¥è¾“å‡ºå®Œæ•´çš„Markdownå†…å®¹ï¼ˆæ— éœ€ä»£ç å—ï¼‰ã€‚"""

    result, metadata = call_ollama(model, prompt)

    if result:
        metadata["category"] = "sun-tzu"
        metadata["user_note_length"] = len(user_note)
        metadata["output_length"] = len(result)

    return result, metadata


def save_annotation(content: str, metadata: dict) -> Path:
    """ä¿å­˜æ³¨ç–åˆ°æ–‡ä»¶"""

    ANNOTATIONS_DIR.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M")
    model_name = metadata.get("model", "unknown").replace(":", "-")
    filename = f"{timestamp}-mistral3-edge-{model_name}.md"
    filepath = ANNOTATIONS_DIR / filename

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(content)
        f.write("\n\n---\n\n")
        f.write("## æ€§èƒ½å…ƒæ•°æ®\n\n")
        f.write("```json\n")
        f.write(json.dumps(metadata, ensure_ascii=False, indent=2))
        f.write("\n```\n")

    return filepath


def save_benchmark(metadata: dict) -> Path:
    """ä¿å­˜æ€§èƒ½åŸºå‡†æ•°æ®"""

    BENCHMARK_DIR.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d")
    filepath = BENCHMARK_DIR / f"{timestamp}-benchmarks.jsonl"

    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(metadata, ensure_ascii=False))
        f.write("\n")

    return filepath


def main():
    print("=" * 70)
    print("  Mistral 3 Edge Agent - è¾¹ç¼˜AIæ¨ç†ç³»ç»Ÿ")
    print("  ã€Šå­™å­å…µæ³•ã€‹2025 AIæ—¶ä»£æ³¨ç– - æœ¬åœ°éšç§ä¼˜å…ˆ")
    print("=" * 70)
    print()

    # æ£€æŸ¥ Ollama
    if not check_ollama_installed():
        print("âŒ Ollama æœªå®‰è£…")
        print("è¯·è®¿é—®: https://ollama.com/download")
        sys.exit(1)

    print("âœ… Ollama å·²å®‰è£…")
    print()

    # é€‰æ‹©æœ€ä½³æ¨¡å‹
    model = select_best_model()

    if not model:
        print("âŒ æ— å¯ç”¨æ¨¡å‹")
        print()
        print("è¯·å°è¯•ï¼š")
        print("1. å‡çº§ Ollama: https://github.com/ollama/ollama/releases")
        print("2. æˆ–æ‹‰å–å¤‡é€‰æ¨¡å‹: ollama pull deepseek-r1:8b")
        sys.exit(1)

    print()
    print("=" * 70)
    print()

    # è·å–ç”¨æˆ·è¾“å…¥
    if len(sys.argv) > 1:
        user_note = " ".join(sys.argv[1:])
    else:
        print("è¯·è¾“å…¥ä»Šå¤©çš„ã€Šå­™å­å…µæ³•ã€‹å¿ƒå¾—ï¼ˆ300-800å­—ï¼‰ï¼š")
        print()
        user_note = input("> ")

    if not user_note.strip():
        print("âŒ å¿ƒå¾—ä¸èƒ½ä¸ºç©º")
        sys.exit(1)

    print()
    print(f"ğŸ“ æ”¶åˆ°å¿ƒå¾— ({len(user_note)} å­—)")
    print()
    print("=" * 70)
    print()

    # ç”ŸæˆAIæ³¨ç–
    annotation, metadata = generate_sun_tzu_annotation(model, user_note)

    if not annotation:
        print("âŒ æ³¨ç–ç”Ÿæˆå¤±è´¥")
        sys.exit(1)

    # ä¿å­˜ç»“æœ
    annotation_path = save_annotation(annotation, metadata)
    benchmark_path = save_benchmark(metadata)

    print("=" * 70)
    print("âœ… å®Œæˆï¼")
    print("=" * 70)
    print()
    print(f"ğŸ“„ æ³¨ç–æ–‡ä»¶: {annotation_path.relative_to(REPO_ROOT)}")
    print(f"ğŸ“Š æ€§èƒ½æ•°æ®: {benchmark_path.relative_to(REPO_ROOT)}")
    print()
    print(f"â±ï¸  è€—æ—¶: {metadata.get('elapsed_seconds', 0)}ç§’")
    print(f"âš¡ é€Ÿåº¦: {metadata.get('tokens_per_second', 0):.1f} tokens/ç§’")
    print(f"ğŸ“ è¾“å‡º: {metadata.get('output_length', 0)} å­—ç¬¦")
    print()

    # é¢„è§ˆ
    print("=" * 70)
    print("ğŸ“– æ³¨ç–é¢„è§ˆï¼ˆå‰600å­—ï¼‰ï¼š")
    print("=" * 70)
    print()
    preview = annotation[:600]
    print(preview)
    if len(annotation) > 600:
        print("\n... (æŸ¥çœ‹å®Œæ•´æ–‡ä»¶ä»¥é˜…è¯»å‰©ä½™å†…å®¹)")
    print()

    print("=" * 70)
    print("ğŸ¯ å¯¹æ¯”åŸºå‡†ï¼šLlama-3.3-70B @ Groq äº‘ç«¯ vs Mistral 3 @ è¾¹ç¼˜")
    print("=" * 70)
    print(f"- è¾¹ç¼˜å»¶è¿Ÿ: {metadata.get('elapsed_seconds', 0)}ç§’ï¼ˆæœ¬æ¬¡ï¼‰")
    print("- è¾¹ç¼˜éšç§: âœ… 100%æœ¬åœ°ï¼Œæ— æ•°æ®ä¸Šäº‘")
    print("- è¾¹ç¼˜æˆæœ¬: âœ… $0/æ¨ç†ï¼ˆvs äº‘ç«¯ $0.50-2/M tokensï¼‰")
    print("- æ¨¡å‹è§„æ¨¡: 8B paramsï¼ˆvs 70Bäº‘ç«¯ï¼‰")
    print()
    print("ğŸ’¡ ç»“è®ºï¼šè¾¹ç¼˜ AI é€‚åˆéšç§åœºæ™¯ã€ç¦»çº¿æ¨ç†ã€æˆæœ¬æ•æ„Ÿåº”ç”¨")
    print()

    # Git æäº¤æç¤º
    print("=" * 70)
    print("ğŸ’¾ ä¸‹ä¸€æ­¥ï¼šæäº¤åˆ°ä»“åº“ï¼Ÿ")
    print("=" * 70)
    print()
    print(f"git add {annotation_path.relative_to(REPO_ROOT)}")
    print(f"git add {benchmark_path.relative_to(REPO_ROOT)}")
    print(f'git commit -m "feat: Add Mistral 3 edge annotation ({datetime.now().strftime('%Y-%m-%d')})"')
    print("git push origin main")
    print()
    print("ğŸš€ 7å¹´ä¼ æ‰¿è®¡åˆ’ - è¾¹ç¼˜AIèµ‹èƒ½")
    print()


if __name__ == "__main__":
    main()
